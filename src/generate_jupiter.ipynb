{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85c6e24-542e-417a-9565-7471732da3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7630, 1400)\n",
      "ERA5 file not found - creating new\n",
      "Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñè                                                                                                                                                                           | 11/7630 [00:03<34:53,  3.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m\n\u001b[1;32m     86\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./artifacts/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_model.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_file)\n\u001b[0;32m---> 90\u001b[0m config, dataset, traffic, conditions \u001b[38;5;241m=\u001b[39m \u001b[43mget_config_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraj_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     92\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous_len\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mcon_conditions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m, in \u001b[0;36mget_config_data\u001b[0;34m(config_path, data_path, artifact_location)\u001b[0m\n\u001b[1;32m     66\u001b[0m configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_path \n\u001b[1;32m     67\u001b[0m configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifact_location\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m artifact_location\n\u001b[0;32m---> 69\u001b[0m dataset, traffic \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m condition_config \u001b[38;5;241m=\u001b[39m configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mconditional_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/syntraj_diffusion/src/utils/helper.py:39\u001b[0m, in \u001b[0;36mload_and_prepare_data\u001b[0;34m(configs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03mLoad and prepare the dataset for the model.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m dataset_config \u001b[38;5;241m=\u001b[39m configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 39\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTrafficDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_shape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMinMaxScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfo_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfo_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditional_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_conditions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweather_grid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvariables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m traffic \u001b[38;5;241m=\u001b[39m Traffic\u001b[38;5;241m.\u001b[39mfrom_file(dataset_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset, traffic\n",
      "File \u001b[0;32m~/syntraj_diffusion/src/utils/data_utils.py:444\u001b[0m, in \u001b[0;36mTrafficDataset.from_file\u001b[0;34m(cls, file_path, features, shape, scaler, info_params, conditional_features, variables)\u001b[0m\n\u001b[1;32m    441\u001b[0m traffic \u001b[38;5;241m=\u001b[39m traffic\u001b[38;5;241m.\u001b[39mbetween(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2018-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2021-12-31\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m#traffic = traffic.between(\"2018-01-01\", \"2018-01-31\")\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraffic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditional_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m dataset\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m file_path\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/syntraj_diffusion/src/utils/data_utils.py:313\u001b[0m, in \u001b[0;36mTrafficDataset.__init__\u001b[0;34m(self, traffic, features, shape, scaler, info_params, conditional_features, variables)\u001b[0m\n\u001b[1;32m    311\u001b[0m num_levels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m#self.grid_conditions = load_weather_data_function(nc_files, traffic, preprocess, save_path, grid_size = grid_size, num_levels=num_levels, pressure_levels = pressure_levels)\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_conditions \u001b[38;5;241m=\u001b[39m \u001b[43mload_weather_data_arrival_airport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnc_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraffic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_levels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpressure_levels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpressure_levels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(traffic) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_conditions)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_conditions))\n",
      "File \u001b[0;32m~/syntraj_diffusion/src/utils/weather_utils.py:207\u001b[0m, in \u001b[0;36mload_weather_data_arrival_airport\u001b[0;34m(file_paths, traffic, variables, save_path, grid_size, num_levels, pressure_levels)\u001b[0m\n\u001b[1;32m    203\u001b[0m     sub \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39msel(time\u001b[38;5;241m=\u001b[39mformatted_timestamp)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Convert the selected subset to a PyTorch FloatTensor, filling NaN values if necessary\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m#data_array = sub.to_array().fillna(0).values  # Filling NaNs with 0 or another appropriate value\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     data_array \u001b[38;5;241m=\u001b[39m \u001b[43msub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[1;32m    208\u001b[0m     grid_conditions\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mFloatTensor(data_array))\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Save the processed grid_conditions as a pickle file\u001b[39;00m\n",
      "File \u001b[0;32m~/syntraj_diffusion/.venv/lib/python3.10/site-packages/xarray/core/dataarray.py:814\u001b[0m, in \u001b[0;36mDataArray.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03m    The array's data converted to numpy.ndarray.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m    to this array may be reflected in the DataArray as well.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[0;32m~/syntraj_diffusion/.venv/lib/python3.10/site-packages/xarray/core/variable.py:566\u001b[0m, in \u001b[0;36mVariable.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    565\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/syntraj_diffusion/.venv/lib/python3.10/site-packages/xarray/core/variable.py:363\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/syntraj_diffusion/.venv/lib/python3.10/site-packages/dask/array/core.py:1746\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1746\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[1;32m   1748\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[0;32m~/syntraj_diffusion/.venv/lib/python3.10/site-packages/dask/base.py:372\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/syntraj_diffusion/.venv/lib/python3.10/site-packages/dask/base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Any, Dict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils import load_config\n",
    "from model.AirDiffTraj import AirDiffTraj, AirDiffTrajDDPM\n",
    "from utils.data_utils import TrafficDataset\n",
    "from traffic.core import Traffic\n",
    "from traffic.algorithms.generation import Generation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils.condition_utils import load_conditions\n",
    "from utils.helper import load_and_prepare_data, get_model\n",
    "\n",
    "def get_checkpoint_path(logger_config: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Get the path to the checkpoint file.\n",
    "    \"\"\"\n",
    "    run_name = logger_config[\"run_name\"]\n",
    "    artifact_location = logger_config[\"artifact_location\"]\n",
    "    # check that the artifact location exists, otherwise raise an error\n",
    "    if not os.path.exists(artifact_location):\n",
    "        raise FileNotFoundError(f\"Artifact directory {artifact_location} not found!\")\n",
    "\n",
    "    artifact_location = os.path.join(artifact_location, run_name)\n",
    "    # check that artifact location exists, otherwise raise an error\n",
    "    if not os.path.exists(artifact_location):\n",
    "        raise FileNotFoundError(f\"Artifact location {artifact_location} not found!\")\n",
    "\n",
    "    # get the \"best_model.ckpt\" file\n",
    "    checkpoint = os.path.join(artifact_location, \"best_model.ckpt\")\n",
    "    # check that the checkpoint exists, otherwise raise an error\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise FileNotFoundError(f\"Checkpoint file {checkpoint} not found!\")\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "def get_models(model_config, dataset_params, checkpoint_path, dataset_scaler):\n",
    "    \"\"\"\n",
    "    Load the trained model and create the trajectory generation model.\n",
    "    \"\"\"\n",
    "    model = get_model(model_config).load_from_checkpoint(checkpoint_path, dataset_params = dataset_params, config = model_config)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print(\"Model loaded with checkpoint!\")\n",
    "\n",
    "    from traffic.algorithms.generation import Generation\n",
    "\n",
    "    trajectory_generation_model = Generation(\n",
    "        generation=model,\n",
    "        features=dataset.parameters['features'],\n",
    "        scaler=dataset_scaler,\n",
    "    )\n",
    "    \n",
    "    print(\"Trajectory generation model created!\")\n",
    "    return model, trajectory_generation_model\n",
    "\n",
    "def get_config_data(config_path: str, data_path: str, artifact_location: str):\n",
    "    configs = load_config(config_path)\n",
    "    configs[\"data\"][\"data_path\"] = data_path \n",
    "    configs[\"logger\"][\"artifact_location\"] = artifact_location\n",
    "    \n",
    "    dataset, traffic = load_and_prepare_data(configs)\n",
    "\n",
    "    condition_config = configs[\"data\"]\n",
    "\n",
    "    if dataset.conditional_features is None:\n",
    "        conditions = load_conditions(condition_config, dataset)\n",
    "    else:\n",
    "        conditions = dataset.conditional_features\n",
    "\n",
    "    return configs, dataset, traffic, conditions\n",
    "\n",
    "\n",
    "\n",
    "config_file = \"./configs/config.yaml\"\n",
    "data_path = \"./data/resampled/combined_traffic_resampled_200.pkl\"\n",
    "model_name = \"AirDiffTraj_17\"\n",
    "artifact_location= \"./artifacts\"\n",
    "checkpoint = f\"./artifacts/{model_name}/best_model.ckpt\"\n",
    "\n",
    "config = load_config(config_file)\n",
    "\n",
    "config, dataset, traffic, conditions = get_config_data(config_file, data_path, artifact_location)\n",
    "config['model'][\"traj_length\"] = dataset.parameters['seq_len']\n",
    "config['model'][\"continuous_len\"] = dataset.con_conditions.shape[1]\n",
    "model, trajectory_generation_model = get_models(config[\"model\"], dataset.parameters, checkpoint, dataset.scaler)\n",
    "dataset_config = config[\"data\"]\n",
    "batch_size = dataset_config[\"batch_size\"]\n",
    "\n",
    "X, con, cat, grid = dataset[3]\n",
    "print(con.shape, cat.shape)\n",
    "\n",
    "\n",
    "# Download and load the training t\n",
    "dataset_config = config[\"data\"]\n",
    "batch_size = dataset_config[\"batch_size\"]\n",
    "#train_dataset = FashionMNIST(root='./data', train=True, transform=transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403b4e2d-aa3c-42c5-b2ef-12f8ad1223f3",
   "metadata": {},
   "source": [
    "# Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257bba7-8b06-44d4-b169-6d40b224a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.randint(0, len(dataset), (30))\n",
    "def generate_samples(dataset, model, k=10, n=10, length=200):\n",
    "    # Initialize lists to store results for each sample\n",
    "    all_samples = []\n",
    "    all_steps = []\n",
    "    #print(rnd)\n",
    "    for i in rnd:\n",
    "        # Load the i-th sample from the dataset\n",
    "        x, con, cat, grid = dataset[i]\n",
    "        \n",
    "        # Reshape con and cat as required\n",
    "        con = con.reshape(1, -1)\n",
    "        cat = cat.reshape(1, -1)\n",
    "        \n",
    "        # Adjust the shape of x\n",
    "        #x = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # Move grid to the device and adjust dimensions\n",
    "        grid = grid.unsqueeze(dim=0).to(\"cuda\")\n",
    "        \n",
    "        # Generate samples and steps using the model\n",
    "        samples, steps = model.sample(n, con, cat, grid, length)\n",
    "        \n",
    "        # Append results to the lists\n",
    "        all_samples.append(samples)\n",
    "        all_steps.append(steps)\n",
    "        \n",
    "        # Print out shapes for verification (optional)\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        #print(\"cat shape:\", cat.shape)\n",
    "        #print(\"grid shape:\", grid.shape)\n",
    "        #print(\"samples shape:\", samples.shape)\n",
    "        #print(\"steps length:\", len(steps))\n",
    "    \n",
    "    return all_samples, all_steps\n",
    "\n",
    "\n",
    "samples, steps = generate_samples(dataset, model, 30, 100)\n",
    "#print(samples)\n",
    "#print(samples.shape)\n",
    "#print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5d71d-f857-4b51-bb57-3a8489080c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82798845-f348-43e7-a907-15e20c076e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import traffic.core as tc  # Ensure the traffic library is installed\n",
    "\n",
    "def detach_to_tensor(tensor_list):\n",
    "    \"\"\"\n",
    "    Detaches each tensor in the list, moves to CPU, and stacks them into a single tensor.\n",
    "    \"\"\"\n",
    "    return np.stack([tensor.cpu().detach() for tensor in tensor_list])\n",
    "\n",
    "def plot_from_array(t):\n",
    "    \"\"\"\n",
    "    Plots data from a traffic.core.Traffic object on a EuroPP projection.\n",
    "    \"\"\"\n",
    "    plt.style.use(\"ggplot\")\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 1, 1, projection=ccrs.EuroPP())\n",
    "    ax1.coastlines()\n",
    "    t.plot(ax1, alpha=0.5, color=\"red\", linewidth=1)\n",
    "    ax1.add_feature(cartopy.feature.BORDERS, linestyle=\":\", alpha=1.0)\n",
    "    plt.xlabel('X values')\n",
    "    plt.ylabel('Y values')\n",
    "    plt.title('Plot of X and Y Data from Array')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'samples' is a list of tensors generated by model.sample\n",
    "# Detach and stack samples into a single tensor\n",
    "detached_samples = detach_to_tensor(samples).reshape(-1, 4, 200)\n",
    "\n",
    "# Confirm detached_samples is a tensor and ready for use\n",
    "print(\"Detached samples tensor shape:\", detached_samples.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d8dc0-dd41-421d-b616-93cb1f487c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic.algorithms.generation import Generation\n",
    "\n",
    "trajectory_generation_model = Generation(\n",
    "    generation=model,\n",
    "    features=dataset.parameters['features'],\n",
    "    scaler=dataset.scaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a16b25-7ed9-46ba-96a7-8fa3b6920660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reco_x = detached_samples.transpose(0, 2, 1).reshape(detached_samples.shape[0], -1)\n",
    "decoded = dataset.scaler.inverse_transform(reco_x)\n",
    "reconstructed_traf = trajectory_generation_model.build_traffic(\n",
    "    decoded,\n",
    "    coordinates=dict(latitude=48.5, longitude=8.4),\n",
    "    forward=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0037b7-4f32-4913-a225-15a6c477256f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reconstructed_traf\n",
    "plot_from_array(reconstructed_traf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb2f798-67f8-4c42-8af3-afc3a9520c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE \n",
    "from sklearn.decomposition import PCA  \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np  \n",
    "\n",
    "def data_diversity(ori_data, generated_data, analysis, average_dimension='sequence', max_sample_size=1000):\n",
    "    \"\"\"\n",
    "    Data diversity assessment using PCA or t-SNE for original & synthetic distribution visualization.\n",
    "\n",
    "    Inputs:\n",
    "        - ori_data (array): original data\n",
    "        - synthetic_data (array): synthetic data\n",
    "        - analysis_type (str): PCA or t-SNE\n",
    "        - average_dimension (string): flatten along 'sequence' or 'samples' dimension\n",
    "        - max_sample_size (int): maximum sample size for computational speed\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    \n",
    "    # Determine the analysis sample size (minimum of 1000 or the length of the original data)\n",
    "    anal_sample_no = min(max_sample_size, len(ori_data))\n",
    "\n",
    "    # Randomly permute indices for data preprocessing\n",
    "    idx = np.random.permutation(len(ori_data))[:anal_sample_no]\n",
    "\n",
    "    # Convert original and generated data to numpy arrays and select a subset based on indices\n",
    "    ori_data, generated_data = np.asarray(ori_data)[idx], np.asarray(generated_data)[idx]\n",
    "\n",
    "    if average_dimension =='sequence':\n",
    "        # Compute the mean along the sequence length dimension for both datasets\n",
    "        prep_data = np.mean(ori_data[:, :, :], axis=1)\n",
    "        prep_data_hat = np.mean(generated_data[:, :, :], axis=1)\n",
    "    \n",
    "    elif average_dimension =='samples':\n",
    "        prep_data = np.mean(ori_data[:, :, :], axis=2)\n",
    "        prep_data_hat = np.mean(generated_data[:, :, :], axis=2)\n",
    "        \n",
    "    else:\n",
    "        prep_data = ori_data.reshape(ori_data.shape[0], -1)\n",
    "        prep_data_hat = generated_data.reshape(generated_data.shape[0], -1)\n",
    "            \n",
    "    # Define colors for visualization (red for original, blue for synthetic)\n",
    "    colors = [\"red\"] * anal_sample_no + [\"blue\"] * anal_sample_no\n",
    "\n",
    "    # Perform analysis based on user choice (PCA or t-SNE)\n",
    "    if analysis == 'PCA':\n",
    "        # Apply PCA to both original and synthetic data\n",
    "        pca_results = PCA(n_components=2).fit_transform(prep_data)\n",
    "        pca_hat_results = PCA(n_components=2).fit_transform(prep_data_hat)\n",
    "\n",
    "        # Plot PCA results\n",
    "        plt.scatter(pca_results[:, 0], pca_results[:, 1], c=colors[:anal_sample_no], alpha=0.35, label=\"Original\")\n",
    "        plt.scatter(pca_hat_results[:, 0], pca_hat_results[:, 1], c=colors[anal_sample_no:], alpha=0.35, label=\"Synthetic\")\n",
    "\n",
    "    elif analysis == 't-SNE':\n",
    "        # Combine preprocessed data for t-SNE analysis\n",
    "        prep_data_final = np.concatenate((prep_data, prep_data_hat), axis=0)\n",
    "\n",
    "        # Apply t-SNE to combined data\n",
    "        tsne_results = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300).fit_transform(prep_data_final)\n",
    "\n",
    "        # Plot t-SNE results\n",
    "        plt.scatter(tsne_results[:anal_sample_no, 0], tsne_results[:anal_sample_no, 1], c=colors[:anal_sample_no], alpha=0.35, label=\"Original\")\n",
    "        plt.scatter(tsne_results[anal_sample_no:, 0], tsne_results[anal_sample_no:, 1], c=colors[anal_sample_no:], alpha=0.35, label=\"Synthetic\")\n",
    "\n",
    "    # Add legend and labels to the plot\n",
    "    # plt.legend()\n",
    "    plt.title('PCA Plot' if analysis == 'PCA' else 't-SNE Plot')\n",
    "    plt.xlabel('x-pca' if analysis == 'PCA' else 'x-tsne')\n",
    "    plt.ylabel('y-pca' if analysis == 'PCA' else 'y-tsne')\n",
    "    plt.savefig('diversity', dpi=400, bbox_inches='tight')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "print(decoded.shape)\n",
    "data_diversity(dataset[:3000][0][:,:2,:], reco_x.reshape(3000, 4, 200)[:,:2,:], 'PCA', 'sequence')\n",
    "data_diversity(dataset[:3000][0][:,:2,:], reco_x.reshape(3000, 4, 200)[:,:2,:], 'PCA', 'samples')\n",
    "data_diversity(dataset[:3000][0][:,:2,:], reco_x.reshape(3000, 4, 200)[:,:2,:], 'PCA')\n",
    "data_diversity(dataset[:3000][0][:,:2,:], reco_x.reshape(3000, 4, 200)[:,:2,:], 't-SNE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527f097-3316-4e52-8af1-8c852181a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def plot_from_array(data):\n",
    "    # Check if the input data has the correct shape\n",
    "    #if data.shape[1] != 2:\n",
    "     #   raise ValueError(\"The second dimension of the array must have a size of 2 for x and y coordinates.\")\n",
    "    \n",
    "  # Length of each x and y data series\n",
    "    n = data.shape[0]\n",
    "    plt.style.use(\"ggplot\")\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for i in range(n):\n",
    "        y = data[i, :, 0]  # Extract the x values for the i-th series\n",
    "        x = data[i, :, 1]  # Extract the y values for the i-th series\n",
    "        \n",
    "        plt.scatter(x, y, label=f'Series {i+1}')  # Plot each series with a label\n",
    "\n",
    "    #ax1 = fig.add_subplot(1, 1, 1)\n",
    "    #ax1.coastlines()\n",
    "    #t.plot(ax1, alpha=0.1, color=\"red\", linewidth=0.2)\n",
    "    #ax1.add_feature(cartopy.feature.BORDERS, linestyle=\":\", alpha=1.0)\n",
    "    plt.xlabel('X values')\n",
    "    plt.ylabel('Y values')\n",
    "    plt.title('Plot of X and Y Data from Array')\n",
    "    plt.show()\n",
    "    \n",
    "scaler = dataset.scaler\n",
    "#print(detached_s.shape)\n",
    "\n",
    "detached_w = detached_s.transpose(0, 2, 1).reshape(detached_s.shape[0], -1)\n",
    "detached_w = scaler.inverse_transform(detached_w).reshape(100, -1, 4)\n",
    "\n",
    "\n",
    "print(detached_w.shape)\n",
    "\n",
    "plot_from_array(detached_w[:1, :, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20899bcc-4b77-4f61-a0e0-537fa76d32eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"for i in steps:\n",
    "    scaler = dataset.scaler\n",
    "    i = i.cpu().numpy()\n",
    "    #print(detached_s.shape)\n",
    "    detached_w = i.transpose(0, 2, 1).reshape(i.shape[0], -1)\n",
    "    detached_w = scaler.inverse_transform(detached_w).reshape(100, -1, 4)\n",
    "    #print(detached_w.shape)\n",
    "    \n",
    "    plot_from_array(detached_w[:, :, :2])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed548b-6254-40f8-9f0d-77e0137e099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import traffic.core as tc\n",
    "def plot_diffusion_steps(steps):\n",
    "    \"\"\"\n",
    "    Plot images stored in a list side-by-side to visualize diffusion steps.\n",
    "    \n",
    "    Args:\n",
    "        images (list): A list of images (each can be a 2D numpy array or a tensor) to be plotted.\n",
    "    \"\"\"\n",
    "    n = len(steps)  # Number of images (or steps) in the diffusion process\n",
    "    \n",
    "    # Create subplots with 1 row and n columns\n",
    "    fig, axes = plt.subplots(1, n, figsize=(20, 10))\n",
    "    \n",
    "    # If only one image is provided, make sure axes is iterable\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each image in a subplot\n",
    "    scaler = dataset.scaler\n",
    "    for e, i in enumerate(steps):\n",
    "        i = i.cpu().numpy()    \n",
    "        detached_w = i.transpose(0, 2, 1).reshape(i.shape[0], -1)\n",
    "        detached_w = scaler.inverse_transform(detached_w).reshape(i.shape[0], -1, 4)\n",
    "        for p in range(n):\n",
    "            y = detached_w[p, :, 0]  # Extract the x values for the i-th series\n",
    "            x = detached_w[p, :, 1]  # Extract the y values for the i-th series\n",
    "            axes[e].scatter(x, y, label=f'Series {i+1}')  # Plot each series with a label\n",
    "            axes[e].axis('off')  # Turn off the axis for clarity\n",
    "            axes[e].set_title(f\"Step {i+1}\")\n",
    "    \n",
    "    # Adjust the spacing between plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_diffusion_steps(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174cfb9c-58c6-4a21-b959-cbc3e5e0c589",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485144ea-bc2c-46a0-88fd-c8dd33b3fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "rnd = np.random.randint(0, len(dataset), (n))\n",
    "X2, con, cat, grid = dataset[rnd]\n",
    "grid = grid.to(\"cuda\")\n",
    "X_ = X2.reshape(n, 4, -1).to(\"cuda\")\n",
    "con_ = con.reshape(n, -1)\n",
    "cat_ = cat.reshape(n, -1)\n",
    "print(con.shape, cat.shape, X_.shape)\n",
    "#model.unet.guidance_scale = 3\n",
    "x_rec, steps = model.reconstruct(X_, con_, cat_, grid)\n",
    "#print(cat.unique(), con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8d1df-eac7-43d2-989d-fab1fd911e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_.shape, x_rec.shape)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import traffic.core as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e4250-74ad-43e9-ae08-9f36e3314c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax1 = fig.add_subplot(1, 1, 1, projection=ccrs.EuroPP())\n",
    "ax1.coastlines()\n",
    "ax1.add_feature(cartopy.feature.BORDERS, linestyle=\":\", alpha=1.0)\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.title('Plot of X and Y Data from Array')\n",
    "print(\"MSE\",torch.nn.functional.mse_loss(X_, x_rec))\n",
    "color = [\"red\", \"blue\"]\n",
    "a = []\n",
    "for c, i in enumerate([X_, x_rec]):\n",
    "    print(i.cpu().numpy().shape)\n",
    "    i = i.cpu().numpy()\n",
    "    reco_x = i.transpose(0, 2, 1).reshape(i.shape[0], -1)\n",
    "    decoded = dataset.scaler.inverse_transform(reco_x)\n",
    "    reconstructed_traf = trajectory_generation_model.build_traffic(\n",
    "    decoded.reshape(n, -1, 4),\n",
    "    coordinates=dict(latitude=48.5, longitude=8.4),\n",
    "    forward=False,\n",
    "    )\n",
    "    a.append(reconstructed_traf)\n",
    "    #for i in range(n):\n",
    "        #x = data[i, 0, :]  # Extract the x values for the i-th series\n",
    "        #y = data[i, 1, :]  # Extract the y values for the i-th series\n",
    "        \n",
    "        #plt.scatter(x, y, label=f'Series {i+1}')  # Plot each series with a label\n",
    "\n",
    "\n",
    "    reconstructed_traf.plot(ax1, alpha=0.5, color=color[c], linewidth=1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f2843-d7cb-4eb1-b65b-72965da18340",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj1 = X_.cpu()  # Shape (length, features)\n",
    "traj2 = x_rec.cpu()  # Shape (length, features)\n",
    "\n",
    "# Compute differences (could be absolute difference or any other metric)\n",
    "differences = torch.abs(traj1 - traj2).mean(dim=1)  # Shape (length, features)\n",
    "differences_max, _ = torch.abs(traj1 - traj2).max(dim=1)\n",
    "differences_min, _ = torch.abs(traj1 - traj2).min(dim=1) \n",
    "print(differences.shape)\n",
    "# Plot the differences for each feature over the length of the trajectory\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(3):\n",
    "    name = [\"longitude\", \"latitude\", \"altitude\"]\n",
    "    plt.plot(differences[i, :].numpy(), label=f'{name[i]}')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Feature-wise comparison between two trajectories MEAN')\n",
    "plt.ylabel('Difference')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(3):\n",
    "    name = [\"longitude\", \"latitude\", \"altitude\"]\n",
    "    plt.plot(differences_max[i, :].numpy(), label=f'{name[i]}')\n",
    "    #plt.plot(differences_min[i, :].numpy(), label=f'{name[i]}')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Feature-wise comparison between two trajectories MAX')\n",
    "plt.ylabel('Difference')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c5ace-58ff-4058-882d-76665866f123",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "## Density Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792deb59-c509-44a6-9ba8-c9e928c7872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from traffic.core import Traffic\n",
    "training_trajectories_path = \"./data/resampled/combined_traffic_resampled_200.pkl\"\n",
    "training_trajectories = Traffic.from_file(training_trajectories_path)\n",
    "#synthetic_trajectories_path = \"../data/synthetic_compare/OpenSky_EHAM_LIMC.pkl\"\n",
    "#synthetic_trajectories = Traffic.from_file(synthetic_trajectories_path)\n",
    "synthetic_trajectories = a[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1edb7-9613-4752-9609-b0a2dabe5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyproj import Geod\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def get_flight_durations(traffic):\n",
    "    durations = []\n",
    "    for flight in traffic:\n",
    "        duration = (flight.data['timestamp'].max() - flight.data['timestamp'].min()).total_seconds() / 60  # in minutes\n",
    "        durations.append(duration)\n",
    "    return np.array(durations)\n",
    "\n",
    "def get_flight_speeds(traffic, method='calculate', remove_outliers=True, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    all_speeds = []\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    \n",
    "    for flight in traffic:\n",
    "        if method == 'groundspeed' and 'groundspeed' in flight.data.columns:\n",
    "            speeds = flight.data['groundspeed'].values\n",
    "        elif method == 'calculate':\n",
    "            coords = np.column_stack((flight.data['longitude'], flight.data['latitude']))\n",
    "            times = flight.data['timestamp'].values\n",
    "            \n",
    "            distances = geod.inv(coords[:-1, 0], coords[:-1, 1], coords[1:, 0], coords[1:, 1])[2]\n",
    "            time_diffs = np.diff(times).astype('timedelta64[s]').astype(float)\n",
    "            \n",
    "            # Filter out invalid or negative time differences\n",
    "            valid_mask = (time_diffs > 0) & (distances >= 0)\n",
    "            valid_distances = distances[valid_mask]\n",
    "            valid_time_diffs = time_diffs[valid_mask]\n",
    "            \n",
    "            # Calculate speeds and convert to km/h\n",
    "            speeds = valid_distances / valid_time_diffs * 3.6  \n",
    "        else:\n",
    "            raise ValueError(\"Method must be either 'groundspeed' or 'calculate'\")\n",
    "        \n",
    "        # Remove zero speeds\n",
    "        speeds = speeds[speeds > 0]\n",
    "        \n",
    "        all_speeds.extend(speeds)\n",
    "    \n",
    "    all_speeds = np.array(all_speeds)\n",
    "    if remove_outliers:\n",
    "        lower_bound = np.quantile(all_speeds, lower_quantile)\n",
    "        upper_bound = np.quantile(all_speeds, upper_quantile)\n",
    "        all_speeds = all_speeds[(all_speeds >= lower_bound) & (all_speeds <= upper_bound)]\n",
    "    \n",
    "    return all_speeds\n",
    "\n",
    "training_durations = get_flight_durations(training_trajectories)\n",
    "synthetic_durations = get_flight_durations(synthetic_trajectories)\n",
    "print(training_trajectories)\n",
    "print(training_trajectories[rnd])\n",
    "\n",
    "small_training_trajectories = a[0]\n",
    "small_synthetic_trajectories = a[1]\n",
    "\n",
    "training_speeds = get_flight_speeds(small_training_trajectories, method='calculate')\n",
    "synthetic_speeds = get_flight_speeds(small_synthetic_trajectories, method='calculate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a311b-5623-41a6-b703-c3f98a18ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress specific UserWarnings related to set_xticklabels\n",
    "warnings.filterwarnings(\"ignore\", message=\".*set_ticklabels.*\")\n",
    "\n",
    "# Set the style for a more professional look\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Create a single figure with two rows and two columns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Row 1: Flight Durations\n",
    "sns.histplot(training_durations, kde=True, element=\"step\", label='Real', color='#1f77b4', linewidth=1.5, ax=axes[0, 0])\n",
    "sns.histplot(synthetic_durations, kde=True, element=\"step\", label='Synthetic', color='#ff7f0e', linewidth=1.5, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Flight Durations', fontsize=10)\n",
    "axes[0, 0].set_xlabel('Duration (minutes)', fontsize=8)\n",
    "axes[0, 0].set_ylabel('Density', fontsize=8)\n",
    "axes[0, 0].legend(fontsize=6)\n",
    "axes[0, 0].tick_params(labelsize=6)\n",
    "\n",
    "sns.boxplot(data=[training_durations, synthetic_durations], palette=['#1f77b4', '#ff7f0e'], ax=axes[0, 1])\n",
    "axes[0, 1].set_xticklabels(['Real', 'Synthetic'], fontsize=8)\n",
    "axes[0, 1].set_title('Flight Durations', fontsize=10)\n",
    "axes[0, 1].set_ylabel('Duration (minutes)', fontsize=8)\n",
    "axes[0, 1].tick_params(labelsize=6)\n",
    "\n",
    "# Row 2: Flight Speeds\n",
    "sns.histplot(training_speeds, kde=True, element=\"step\", label='Real', color='#1f77b4', linewidth=1.5, ax=axes[1, 0])\n",
    "sns.histplot(synthetic_speeds, kde=True, element=\"step\", label='Synthetic', color='#ff7f0e', linewidth=1.5, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Flight Speeds', fontsize=10)\n",
    "axes[1, 0].set_xlabel('Speed (km/h)', fontsize=8)\n",
    "axes[1, 0].set_ylabel('Density', fontsize=8)\n",
    "axes[1, 0].legend(fontsize=6)\n",
    "axes[1, 0].tick_params(labelsize=6)\n",
    "\n",
    "sns.boxplot(data=[training_speeds, synthetic_speeds], palette=['#1f77b4', '#ff7f0e'], ax=axes[1, 1])\n",
    "axes[1, 1].set_xticklabels(['Real', 'Synthetic'], fontsize=8)\n",
    "axes[1, 1].set_title('Flight Speeds', fontsize=10)\n",
    "axes[1, 1].set_ylabel('Speed (km/h)', fontsize=8)\n",
    "axes[1, 1].tick_params(labelsize=6)\n",
    "\n",
    "# Adjust layout and remove top and right spines\n",
    "plt.tight_layout()\n",
    "for ax in axes.flatten():\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add a main title for the entire figure\n",
    "# fig.suptitle('Comparison of Real and Synthetic Flight Data', fontsize=12, y=1.02)\n",
    "\n",
    "# Save the figure\n",
    "#plt.savefig(f\"../.figures/distribution_plots_{route_name}.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075e5ce-4a83-410b-890a-4f5426460697",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67322b81-b60b-40d6-98ba-090fe5cb5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def timeseries_plot(\n",
    "    real_traffic,\n",
    "    synthetic_traffic,\n",
    "    features: list,\n",
    "    units: dict,\n",
    "    n_plot_samples: int = 1000,\n",
    "    alpha: float = 0.3\n",
    "):\n",
    "    # Set the style for a more professional look\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, len(features), figsize=(4 * len(features), 6), sharex=True)\n",
    "\n",
    "    # Prepare data\n",
    "    datasets = [real_traffic, synthetic_traffic]\n",
    "    dataset_names = ['Real', 'Synthetic']\n",
    "    colors = ['#1f77b4', '#ff7f0e']  # Specified colors\n",
    "\n",
    "    for feature_idx, feature in enumerate(features):\n",
    "        # Top row: individual trajectories\n",
    "        for dataset_idx, (dataset, color) in enumerate(zip(datasets, colors)):\n",
    "            feature_data = np.array([flight.data[feature].values for flight in dataset])\n",
    "            sample_ind = np.random.randint(0, len(feature_data), min(n_plot_samples, len(feature_data)))\n",
    "            for idx in sample_ind:\n",
    "                axes[0, feature_idx].plot(feature_data[idx], alpha=alpha, color=color)\n",
    "\n",
    "        axes[0, feature_idx].set_title(f\"{feature.capitalize()}\", fontsize=12)\n",
    "        axes[0, feature_idx].tick_params(labelsize=10)\n",
    "        axes[0, feature_idx].set_ylabel(f\"{feature.capitalize()} ({units[feature]})\", fontsize=12)\n",
    "\n",
    "        # Bottom row: mean and confidence intervals\n",
    "        for dataset_idx, (dataset, color) in enumerate(zip(datasets, colors)):\n",
    "            feature_data = np.array([flight.data[feature].values for flight in dataset])\n",
    "            mean_data = np.mean(feature_data, axis=0)\n",
    "            std_data = np.std(feature_data, axis=0)\n",
    "\n",
    "            axes[1, feature_idx].plot(mean_data, color=color, linewidth=2)\n",
    "\n",
    "            t_value = stats.t.ppf(0.975, df=len(feature_data)-1)\n",
    "            ci = t_value * std_data * np.sqrt(1 + 1/len(feature_data))\n",
    "            axes[1, feature_idx].fill_between(\n",
    "                range(len(mean_data)),\n",
    "                mean_data - ci,\n",
    "                mean_data + ci,\n",
    "                color=color,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "        axes[1, feature_idx].set_xlabel(\"Time Steps\", fontsize=12)\n",
    "        axes[1, feature_idx].set_ylabel(f\"{feature.capitalize()} ({units[feature]})\", fontsize=12)\n",
    "        axes[1, feature_idx].tick_params(labelsize=10)\n",
    "\n",
    "        # Remove top and right spines for both rows\n",
    "        for row in range(2):\n",
    "            axes[row, feature_idx].spines['top'].set_visible(False)\n",
    "            axes[row, feature_idx].spines['right'].set_visible(False)\n",
    "\n",
    "    # Create custom legend elements\n",
    "    legend_elements = []\n",
    "    for color, name in zip(colors, dataset_names):\n",
    "        legend_elements.append(plt.Line2D([0], [0], color=color, lw=3, label=f'{name}'))\n",
    "        legend_elements.append(Patch(facecolor=color, edgecolor=color, alpha=alpha, label=f'{name} 95% CI'))\n",
    "\n",
    "    # Add a single legend for the entire figure\n",
    "    fig.legend(handles=legend_elements, loc='upper center', ncol=2, fontsize=12, bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    #plt.savefig(f\"../.figures/timeseries_ci_{route_name}.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "features_to_plot = ['latitude', 'longitude', 'altitude', 'timedelta']\n",
    "units = {\n",
    "    'latitude': '¬∞',\n",
    "    'longitude': '¬∞',\n",
    "    'altitude': 'ft',\n",
    "    'timedelta': 's'\n",
    "}\n",
    "\n",
    "timeseries_plot(\n",
    "    training_trajectories,\n",
    "    synthetic_trajectories,\n",
    "    features=features_to_plot,\n",
    "    units=units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767223c3-a595-484d-88b0-fbb14318d387",
   "metadata": {},
   "source": [
    "## Jensen-Shannon Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853a58d-b6cb-4128-8980-eb1525d021ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.special import rel_entr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_2 is a traffic.core.Traffic object containing trajectories\n",
    "# Split the traffic object into two halves based on a criterion\n",
    "df_subset1 = a[0]\n",
    "df_subset2 = a[1]\n",
    "\n",
    "# Convert the first subset to a DataFrame and extract lat/lon\n",
    "subset1_data = df_subset1.data[['latitude', 'longitude']].dropna().values\n",
    "subset2_data = df_subset2.data[['latitude', 'longitude']].dropna().values\n",
    "\n",
    "# Kernel Density Estimation (KDE) for both subsets\n",
    "kde_subset1 = gaussian_kde(subset1_data.T)\n",
    "kde_subset2 = gaussian_kde(subset2_data.T)\n",
    "\n",
    "# Create grid to evaluate KDEs over a common region (latitude, longitude)\n",
    "xgrid, ygrid = np.mgrid[\n",
    "    min(subset1_data[:, 0].min(), subset2_data[:, 0].min()):max(subset1_data[:, 0].max(), subset2_data[:, 0].max()):100j,\n",
    "    min(subset1_data[:, 1].min(), subset2_data[:, 1].min()):max(subset1_data[:, 1].max(), subset2_data[:, 1].max()):100j\n",
    "]\n",
    "\n",
    "grid_coords = np.vstack([xgrid.ravel(), ygrid.ravel()])\n",
    "\n",
    "# Evaluate the KDEs on the grid\n",
    "subset1_density = kde_subset1(grid_coords).reshape(100, 100)\n",
    "subset2_density = kde_subset2(grid_coords).reshape(100, 100)\n",
    "\n",
    "# Normalize densities to ensure they sum to 1 (turn them into probabilities)\n",
    "subset1_density /= np.sum(subset1_density)\n",
    "subset2_density /= np.sum(subset2_density)\n",
    "\n",
    "# Add a small constant to avoid zeros in the densities\n",
    "epsilon = 1e-10\n",
    "subset1_density += epsilon\n",
    "subset2_density += epsilon\n",
    "\n",
    "# Compute the average distribution M\n",
    "M = 0.5 * (subset1_density + subset2_density)\n",
    "\n",
    "# Calculate Jensen-Shannon distance using the scipy.spatial.distance.jensenshannon method\n",
    "js_distance = jensenshannon(subset1_density.ravel(), subset2_density.ravel(), base=2)\n",
    "kl_divergence = np.sum(rel_entr(subset1_density, subset2_density))\n",
    "\n",
    "print(f\"KL Divergence between the two subsets: {kl_divergence}\")\n",
    "\n",
    "print(f\"Jensen-Shannon Distance between the two subsets: {js_distance}\")\n",
    "\n",
    "# Plotting the KDEs for comparison\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(subset1_density, origin='lower', cmap='Blues', extent=[xgrid.min(), xgrid.max(), ygrid.min(), ygrid.max()])\n",
    "ax[0].set_title(\"Subset 1 Density\")\n",
    "\n",
    "ax[1].imshow(subset2_density, origin='lower', cmap='Reds', extent=[xgrid.min(), xgrid.max(), ygrid.min(), ygrid.max()])\n",
    "ax[1].set_title(\"Subset 2 Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf52e36-8ef2-4fcb-8553-c81fa496b9a7",
   "metadata": {},
   "source": [
    "# Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b82d5-aecb-4043-b92b-0e239eb97b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from cartopy.crs import EuroPP, PlateCarree\n",
    "from cartes.utils.features import countries, ocean\n",
    "\n",
    "\n",
    "with plt.style.context(\"traffic\"):\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10), frameon=False)\n",
    "    ax = fig.subplots(1, 2, subplot_kw=dict(projection=EuroPP()))\n",
    "\n",
    "    for ax_ in ax:\n",
    "        ax_.add_feature(countries(scale=\"10m\", linewidth=1.5))\n",
    "\n",
    "    vmax = None  # this trick will keep the same colorbar scale for both maps\n",
    "\n",
    "    for i, data in enumerate([df_subset1, df_subset2]):\n",
    "        # Aggregate and query the data, then convert to xarray\n",
    "        data_xarray = data.agg_latlon(\n",
    "            # 10 points per integer lat/lon\n",
    "            resolution=dict(latitude=10, longitude=10),\n",
    "            # count the number of flights\n",
    "            flight_id=\"nunique\"\n",
    "        ).query(f\"flight_id > 1\").to_xarray()\n",
    "\n",
    "        # Sort the DataArray by latitude and longitude\n",
    "        data_xarray = data_xarray.sortby(['latitude', 'longitude'])\n",
    "\n",
    "        # Plot the data using pcolormesh\n",
    "        cax = data_xarray.flight_id.plot.pcolormesh(\n",
    "            ax=ax[i],\n",
    "            cmap=\"viridis\",\n",
    "            transform=PlateCarree(),\n",
    "            vmax=vmax,\n",
    "            add_colorbar=False,\n",
    "        )\n",
    "\n",
    "        cbaxes = inset_axes(ax[i], \"4%\", \"60%\", loc=3)\n",
    "        cb = fig.colorbar(cax, cax=cbaxes)\n",
    "\n",
    "        # Keep this value to scale the colorbar for the second day\n",
    "        vmax = cb.vmax\n",
    "\n",
    "        text = AnchoredText(\n",
    "            f\"{data.start_time:%B %d, %Y}\",\n",
    "            loc=1,\n",
    "            prop={\"size\": 24, \"fontname\": \"Ubuntu\"},\n",
    "            frameon=True,\n",
    "        )\n",
    "        text.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "        ax[i].add_artist(text)\n",
    "\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dba248-a42e-45fa-9c11-825ea4b64654",
   "metadata": {},
   "source": [
    "## Length Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc7186-c310-43af-9b54-bfc3762b1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def haversine(lat1, lon1,alt1, lat2, lon2, alt2):\n",
    "    \"\"\"\n",
    "    Compute the great-circle distance between two points on the Earth's surface using their lat/lon values.\n",
    "    Uses the geodesic method from the geopy library, which accounts for Earth's shape.\n",
    "    \n",
    "    Parameters:\n",
    "    - lat1, lon1: Latitude and longitude of the first point\n",
    "    - lat2, lon2: Latitude and longitude of the second point\n",
    "    \n",
    "    Returns:\n",
    "    - Distance between the two points in meters.\n",
    "    \"\"\"\n",
    "    return geodesic((lat1, lon1, alt1), (lat2, lon2, alt2)).meters\n",
    "\n",
    "\n",
    "n = 100\n",
    "X2, con, cat = dataset[:n]\n",
    "X_ = X2.reshape(n, 4, -1).to(\"cuda\")\n",
    "con_ = con.reshape(n, -1)\n",
    "cat_ = cat.reshape(n, -1)\n",
    "print(con.shape, cat.shape, X_.shape)\n",
    "model.unet.guidance_scale = 20\n",
    "x_rec, steps = model.reconstruct(X_, con_, cat_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64766ffa-0601-4d47-907f-b65f3311f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(X):\n",
    "    x_rec = X.cpu().numpy()\n",
    "    reco_x = x_rec.transpose(0, 2, 1).reshape(X.shape[0], -1)\n",
    "    decoded = dataset.scaler.inverse_transform(reco_x)\n",
    "    reconstructed_traf = trajectory_generation_model.build_traffic(\n",
    "    decoded.reshape(n, -1, 4),\n",
    "    coordinates=dict(latitude=48.5, longitude=8.4),\n",
    "    forward=False,\n",
    "    )\n",
    "    return reconstructed_traf\n",
    "X_ = build(X_)\n",
    "x_rec = build(x_rec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
